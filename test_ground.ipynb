{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted duck to duck.fa\n",
      "Extracted leopard to leopard.fa\n",
      "Extracted leishan-spiny-toad to leishan-spiny-toad.fa\n",
      "Extracted emu to emu.fa\n",
      "Extracted common-wall-lizard to common-wall-lizard.fa\n",
      "Extracted beluga to beluga.fa\n",
      "Extracted chicken to chicken.fa\n",
      "Error processing dezipped: [Errno 21] Is a directory: '/home/vassilis/bigdrive/vassilis/DNA_data/dezipped'\n",
      "Extracted clown-anemonefish to clown-anemonefish.fa\n",
      "Extracted two-toed-sloth to two-toed-sloth.fa\n",
      "Extracted guppy to guppy.fa\n",
      "Extracted blue-whale to blue-whale.fa\n",
      "Extracted goodes-thornscrub-tortoise to goodes-thornscrub-tortoise.fa\n",
      "Extracted human to human.fa\n",
      "Extracted clawed-frog to clawed-frog.fa\n",
      "Extracted golden-eagle to golden-eagle.fa\n",
      "Extracted indian-cobra to indian-cobra.fa\n",
      "Extracted chimpanzee to chimpanzee.fa\n",
      "Extracted narwhal to narwhal.fa\n",
      "Extracted komodo-dragon to komodo-dragon.fa\n",
      "Extracted large-yellow-croaker to large-yellow-croaker.fa\n"
     ]
    }
   ],
   "source": [
    "from modules.util import *\n",
    "\n",
    "folder_files = '/home/vassilis/bigdrive/vassilis/DNA_data'\n",
    "folder_results = '/home/vassilis/bigdrive/vassilis/DNA_data/dezipped'\n",
    "\n",
    "extract_gz(folder_files, folder_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Created empty dataset\n",
      "Tensor shape: torch.Size([38134400240])\n",
      "snippet ggtcgagctctggttttccggaactcgtat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38134400240/38134400240 [11:37<00:00, 54685480.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from modules.tok_utils import make_h5\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yo=SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char_R.pt')\n",
    "\n",
    "make_h5('tokendata/char_ordered_dna_R.h5',tokenizer=yo,data_name='char_ordered_dna_R',destination_folder='h5chardna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4}\n",
      "Got : tok_dict_rev {0: '$', 1: 'A', 2: 'C', 3: 'G', 4: 'T'}\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yo=SimpleTokenizer(tok_dict_loc='tokenizers/DNA_tok.pt')\n",
    "yo2 = SimpleTokenizer(tok_dict_loc='test_toki/DNA_tok.pt')\n",
    "a = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals.h5',attn_length=256)\n",
    "ar = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals.h5',attn_length=256,backwards=True)\n",
    "b = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals_R.h5',attn_length=256,backwards=False)\n",
    "br = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals_R.h5',attn_length=256,backwards=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AAGTTACAAAATATGCAGTAACAAACACAGTACAGATTATTCTTAAGCTAAAAACAGTTTGAAGGAACATTCTTCAGAAAGCTATGTATTTGTATTGCTTGAGAAACAGCTTTAGAAAAATATACAGAATTGTTACAGTTGCTACACACAAACATTAACATTTCCAACTAGGGACTCGAGCAGAAGTTGCTATGTCTCTCCTGCTTCACATTGTTTCCTGGCCAAGATTTCCTCTCATCCAGTTTAGCTTGTGCT\n",
      "$TTCGTGTTCGATTTGACCTACTCTCCTTTAGAACCGGTCCTTTGTTACACTTCGTCCTCTCTGTATCGTTGAAGACGAGCTCAGGGATCAACCTTTACAATTACAAACACACATCGTTGACATTGTTAAGACATATAAAAAGATTTCGACAAAGAGTTCGTTATGTTTATGTATCGAAAGACTTCTTACAAGGAAGTTTGACAAAAATCGAATTCTTATTAGACATGACACAAACAATGACGTATAAAACATTGA\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(a[0][0]))\n",
    "print(yo.detokenize(ar[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AATAGGAATAGGAGTTAAGTCTACATATCGTCCTTTGACAGACCCACAATAAGTGACCTTCTCCTACTAATGTGCACTTATACGATCAAGAACGGGTAGAGATTCGAGTTGTGACGAAAAGATCTTCTCGGTCGAGATGGTCGACCTTGTATGGTCTCTGGAGTCCTTCGGATCTCCAAGGTAATTGAAGATACTAACCCGGTAAAAGGAAACGAAAGTCTCAGCTGTCTTAAGACTTGGAGTTTTCCTTCACTA\n",
      "$CTTCCTTTCATAGAGCTGGTTTGAAATACTCTTTTTGTAATATTTGGAAGTGGACATTGGCAGCGCTTTGAAGCCTATGTTGAAAATGGAAATATCTTCTCCTAAAAACCAGACAGAAGCATTCTCAGAAACTTCCTTGTGATGTGTGTACTCAAGTAACAGAGTTGAACCTTACTTTTGACAGAGCCGTTTTGAAACAGTCTTTTTGTAGAATCTGGAAGTAGATATTTGGATACCTTTGAGGATTTCTTTGGA\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(b[1][0]))\n",
    "print(yo.detokenize(a[-4][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287148136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Dataset contains 38134.40M tokens, resulting in 635573k examples.\n",
      "Dataset contains 38134.40M tokens, resulting in 635573k examples.\n"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yor=SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char_R.pt')\n",
    "yo = SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char.pt')\n",
    "a = TokenTextBOS('h5chardna/char_ordered_dna.h5',attn_length=61+61)\n",
    "ar = TokenTextBOS('h5chardna/char_ordered_dna_R.h5',attn_length=61*2,backwards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$tgatcccaaggcggttcctacatggaacttccaagaggc\n",
      "tctttgccatggtggtgttggaaagctgcagttccctgaagacggtccagggaggcccag\n",
      "gtgcacaccaacacggcaag\n",
      "##\n",
      "agaaagaggaggggaggtggaggaggaagaggaggaaaaagaggaagaggaagaggagga\n",
      "agaggaagaggaagaggaagaggaagaggaagagaaggaacagaaggaggaagaagagga\n",
      "\n",
      "$gcatgttttcaaagagaatg\n",
      "gcagcatcttcccaaccctgaatattcctaaggaagcacgatcctcagcagaggaaaaga\n",
      "ggcttcccatcctatgctcaaggccttttggtctcgagctg\n",
      "##\n",
      "gcatgttttcaaagagaatg\n",
      "gcagcatcttcccaaccctgaatattcctaaggaagcacgatcctcagcagaggaaaaga\n",
      "ggcttcccatcctatgctcaaggccttttggtctcgagctgg\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(a[-40][0]))\n",
    "print('##')\n",
    "print(yo.detokenize(a[0][1]))\n",
    "print(yo.detokenize(ar[0][0]))\n",
    "print('##')\n",
    "print(yo.detokenize(ar[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635573336"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38134400228,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1, 3, 2, 4, 3, 4, 2, 1, 1, 3, 1, 1, 1, 4, 4, 2, 2, 1, 4,\n",
       "       3, 1, 3, 4, 6, 1, 4, 4, 4, 1, 4, 1, 4, 3, 3, 3, 1, 3, 1, 1, 2, 1,\n",
       "       4, 4, 4, 1, 2, 2, 1, 4, 4, 3, 4, 2, 1, 2, 1, 3, 1, 4, 2, 1],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text_tensor[(635573336-1)*61:635573336+63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('ggcttcccatcctatgctcaaggccttttggtctcgagctg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open your HDF5 file\n",
    "with h5py.File('h5chardna/char_ordered_dna.h5', 'r+') as file:\n",
    "    # Read the dataset\n",
    "    dset = file['tokens'][:]\n",
    "    # Slice the dataset to exclude the last 40 entries\n",
    "    sliced_dset = dset[:-40]\n",
    "    \n",
    "    # Delete the original dataset from the file\n",
    "    del file['your_dataset_name']\n",
    "    # Save the sliced dataset back to the file\n",
    "    file.create_dataset('your_dataset_name', data=sliced_dset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
