{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted duck to duck.fa\n",
      "Extracted leopard to leopard.fa\n",
      "Extracted leishan-spiny-toad to leishan-spiny-toad.fa\n",
      "Extracted emu to emu.fa\n",
      "Extracted common-wall-lizard to common-wall-lizard.fa\n",
      "Extracted beluga to beluga.fa\n",
      "Extracted chicken to chicken.fa\n",
      "Error processing dezipped: [Errno 21] Is a directory: '/home/vassilis/bigdrive/vassilis/DNA_data/dezipped'\n",
      "Extracted clown-anemonefish to clown-anemonefish.fa\n",
      "Extracted two-toed-sloth to two-toed-sloth.fa\n",
      "Extracted guppy to guppy.fa\n",
      "Extracted blue-whale to blue-whale.fa\n",
      "Extracted goodes-thornscrub-tortoise to goodes-thornscrub-tortoise.fa\n",
      "Extracted human to human.fa\n",
      "Extracted clawed-frog to clawed-frog.fa\n",
      "Extracted golden-eagle to golden-eagle.fa\n",
      "Extracted indian-cobra to indian-cobra.fa\n",
      "Extracted chimpanzee to chimpanzee.fa\n",
      "Extracted narwhal to narwhal.fa\n",
      "Extracted komodo-dragon to komodo-dragon.fa\n",
      "Extracted large-yellow-croaker to large-yellow-croaker.fa\n"
     ]
    }
   ],
   "source": [
    "from modules.util import *\n",
    "\n",
    "folder_files = '/home/vassilis/bigdrive/vassilis/DNA_data'\n",
    "folder_results = '/home/vassilis/bigdrive/vassilis/DNA_data/dezipped'\n",
    "\n",
    "extract_gz(folder_files, folder_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11862284it [00:11, 1003387.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misbehaved lines :  2322\n",
      "So, 0.019574645152653572% of misbehaved lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.tok_utils import lower_text, k_div_text\n",
    "\n",
    "k_div_text('dataloc/guppy.fa.preprocessed_lower.txt',6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Created empty dataset\n",
      "Tensor shape: torch.Size([38134400240])\n",
      "snippet ggtcgagctctggttttccggaactcgtat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38134400240/38134400240 [11:37<00:00, 54685480.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from modules.tok_utils import make_h5\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yo=SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char_R.pt')\n",
    "\n",
    "make_h5('tokendata/char_ordered_dna_R.h5',tokenizer=yo,data_name='char_ordered_dna_R',destination_folder='h5chardna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4}\n",
      "Got : tok_dict_rev {0: '$', 1: 'A', 2: 'C', 3: 'G', 4: 'T'}\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n",
      "Dataset contains 36467.81M tokens, resulting in 287148k examples.\n"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yo=SimpleTokenizer(tok_dict_loc='tokenizers/DNA_tok.pt')\n",
    "yo2 = SimpleTokenizer(tok_dict_loc='test_toki/DNA_tok.pt')\n",
    "a = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals.h5',attn_length=256)\n",
    "ar = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals.h5',attn_length=256,backwards=True)\n",
    "b = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals_R.h5',attn_length=256,backwards=False)\n",
    "br = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/all_animals_R.h5',attn_length=256,backwards=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AAGTTACAAAATATGCAGTAACAAACACAGTACAGATTATTCTTAAGCTAAAAACAGTTTGAAGGAACATTCTTCAGAAAGCTATGTATTTGTATTGCTTGAGAAACAGCTTTAGAAAAATATACAGAATTGTTACAGTTGCTACACACAAACATTAACATTTCCAACTAGGGACTCGAGCAGAAGTTGCTATGTCTCTCCTGCTTCACATTGTTTCCTGGCCAAGATTTCCTCTCATCCAGTTTAGCTTGTGCT\n",
      "$TTCGTGTTCGATTTGACCTACTCTCCTTTAGAACCGGTCCTTTGTTACACTTCGTCCTCTCTGTATCGTTGAAGACGAGCTCAGGGATCAACCTTTACAATTACAAACACACATCGTTGACATTGTTAAGACATATAAAAAGATTTCGACAAAGAGTTCGTTATGTTTATGTATCGAAAGACTTCTTACAAGGAAGTTTGACAAAAATCGAATTCTTATTAGACATGACACAAACAATGACGTATAAAACATTGA\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(a[0][0]))\n",
    "print(yo.detokenize(ar[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AATAGGAATAGGAGTTAAGTCTACATATCGTCCTTTGACAGACCCACAATAAGTGACCTTCTCCTACTAATGTGCACTTATACGATCAAGAACGGGTAGAGATTCGAGTTGTGACGAAAAGATCTTCTCGGTCGAGATGGTCGACCTTGTATGGTCTCTGGAGTCCTTCGGATCTCCAAGGTAATTGAAGATACTAACCCGGTAAAAGGAAACGAAAGTCTCAGCTGTCTTAAGACTTGGAGTTTTCCTTCACTA\n",
      "$CTTCCTTTCATAGAGCTGGTTTGAAATACTCTTTTTGTAATATTTGGAAGTGGACATTGGCAGCGCTTTGAAGCCTATGTTGAAAATGGAAATATCTTCTCCTAAAAACCAGACAGAAGCATTCTCAGAAACTTCCTTGTGATGTGTGTACTCAAGTAACAGAGTTGAACCTTACTTTTGACAGAGCCGTTTTGAAACAGTCTTTTTGTAGAATCTGGAAGTAGATATTTGGATACCTTTGAGGATTTCTTTGGA\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(b[1][0]))\n",
    "print(yo.detokenize(a[-4][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287148136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Got : tok_dict {'$': 0, 'a': 1, 'c': 2, 'g': 3, 't': 4, 'n': 5, '\\n': 6}\n",
      "Got : tok_dict_rev {0: '$', 1: 'a', 2: 'c', 3: 'g', 4: 't', 5: 'n', 6: '\\n'}\n",
      "Dataset contains 38134.40M tokens, resulting in 635573k examples.\n",
      "Dataset contains 38134.40M tokens, resulting in 635573k examples.\n"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SimpleTokenizer\n",
    "\n",
    "yor=SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char_R.pt')\n",
    "yo = SimpleTokenizer(tok_dict_loc='new_toki_char/ordered_dna_char.pt')\n",
    "a = TokenTextBOS('h5chardna/char_ordered_dna.h5',attn_length=61+61)\n",
    "ar = TokenTextBOS('h5chardna/char_ordered_dna_R.h5',attn_length=61*2,backwards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$tgatcccaaggcggttcctacatggaacttccaagaggc\n",
      "tctttgccatggtggtgttggaaagctgcagttccctgaagacggtccagggaggcccag\n",
      "gtgcacaccaacacggcaag\n",
      "##\n",
      "agaaagaggaggggaggtggaggaggaagaggaggaaaaagaggaagaggaagaggagga\n",
      "agaggaagaggaagaggaagaggaagaggaagagaaggaacagaaggaggaagaagagga\n",
      "\n",
      "$gcatgttttcaaagagaatg\n",
      "gcagcatcttcccaaccctgaatattcctaaggaagcacgatcctcagcagaggaaaaga\n",
      "ggcttcccatcctatgctcaaggccttttggtctcgagctg\n",
      "##\n",
      "gcatgttttcaaagagaatg\n",
      "gcagcatcttcccaaccctgaatattcctaaggaagcacgatcctcagcagaggaaaaga\n",
      "ggcttcccatcctatgctcaaggccttttggtctcgagctgg\n"
     ]
    }
   ],
   "source": [
    "print(yo.detokenize(a[-40][0]))\n",
    "print('##')\n",
    "print(yo.detokenize(a[0][1]))\n",
    "print(yo.detokenize(ar[0][0]))\n",
    "print('##')\n",
    "print(yo.detokenize(ar[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635573336"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38134400228,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1, 3, 2, 4, 3, 4, 2, 1, 1, 3, 1, 1, 1, 4, 4, 2, 2, 1, 4,\n",
       "       3, 1, 3, 4, 6, 1, 4, 4, 4, 1, 4, 1, 4, 3, 3, 3, 1, 3, 1, 1, 2, 1,\n",
       "       4, 4, 4, 1, 2, 2, 1, 4, 4, 3, 4, 2, 1, 2, 1, 3, 1, 4, 2, 1],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text_tensor[(635573336-1)*61:635573336+63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('ggcttcccatcctatgctcaaggccttttggtctcgagctg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open your HDF5 file\n",
    "with h5py.File('h5chardna/char_ordered_dna.h5', 'r+') as file:\n",
    "    # Read the dataset\n",
    "    dset = file['tokens'][:]\n",
    "    # Slice the dataset to exclude the last 40 entries\n",
    "    sliced_dset = dset[:-40]\n",
    "    \n",
    "    # Delete the original dataset from the file\n",
    "    del file['your_dataset_name']\n",
    "    # Save the sliced dataset back to the file\n",
    "    file.create_dataset('your_dataset_name', data=sliced_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixer tokenizer tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tokenizer\n",
      "Generated 6-mer dictionary, length :  15627\n",
      "Dataset contains 11.00M tokens, resulting in 2749k examples.\n",
      "tensor([  0, 255, 255, 255, 255, 255, 255, 255, 255, 255])\n",
      "$aagaataagaataagaataagaataagaataagaataagaataagaataagaat\n",
      "aagaataagaataagaataagaataagaataagaataagaataagaataagaataagaat\n"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SixerTokenizer\n",
    "\n",
    "toki = SixerTokenizer()\n",
    "data = TokenTextBOS('dataloc/h5data/guppysixer.h5.h5',attn_length=10)\n",
    "\n",
    "print(data[0][0])\n",
    "print(toki.detokenize(data[0][0]))\n",
    "print(toki.detokenize(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "saved_tends = torch.load('prep_utils/tokendata/all_6div/Data1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.tok_utils import make_h5\n",
    "from modules.tokenizer import SixerTokenizer\n",
    "\n",
    "toki = SixerTokenizer()\n",
    "\n",
    "make_h5('tokendata/all_6div/',tokenizer=toki,data_name='all_6div',destination_folder='int16ver',dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tokenizer\n",
      "Generated 6-mer dictionary, length :  15627\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File/Folder dataloc/h5data/human_six.h5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      5\u001b[0m toki \u001b[38;5;241m=\u001b[39m SixerTokenizer()\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mTokenTextBOS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataloc/h5data/human_six.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mattn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m310\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m i \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[i][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/DNA/modules/dataset/TokenText.py:32\u001b[0m, in \u001b[0;36mTokenTextBOS.__init__\u001b[0;34m(self, h5_file, attn_length, stride, backwards)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m=\u001b[39m stride\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file)):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile/Folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: File/Folder dataloc/h5data/human_six.h5 not found"
     ]
    }
   ],
   "source": [
    "from modules.dataset import TokenTextBOS\n",
    "from modules.tokenizer import SixerTokenizer\n",
    "import random\n",
    "\n",
    "toki = SixerTokenizer()\n",
    "data = TokenTextBOS('/rcp/csft_scratch/vassilis/DNA_perp/human_six.h5',attn_length=310)\n",
    "i = random.randint(0,len(data))\n",
    "print(data[i][0])\n",
    "print('QUESTION : ')\n",
    "print(toki.detokenize(data[i][0]))\n",
    "print('ANSWER : ')\n",
    "print(toki.detokenize(data[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test attention vs flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "from modules.models.mingpt.MinGPT import CausalSelfAttention,SlowCausalSelfAttention, compare_forwards, compare_speed\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tester = CausalSelfAttention(embed_dim=10,n_heads=5,attn_length=5,dropout=0.)\n",
    "tester2 = SlowCausalSelfAttention(embed_dim=10,n_heads=5,attn_length=5,dropout=0.)\n",
    "# Show parameter number : \n",
    "print(sum(p.numel() for p in tester.parameters()))\n",
    "print(sum(p.numel() for p in tester2.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow attention : 1000 batches of size 128 took 12.92 seconds\n",
      "Fast attention : 1000 batches of size 128 took 10.72 seconds\n"
     ]
    }
   ],
   "source": [
    "compare_speed(device='cuda',embed_dim=256,n_heads=8,attn_length=256,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.1704,  0.2031,  0.1450,  0.0516, -0.1007,  0.2065, -0.1367,\n",
       "           0.0156,  0.0407,  0.2146],\n",
       "         [-0.1703,  0.2269,  0.1584,  0.0454, -0.1002,  0.2149, -0.1540,\n",
       "           0.0192,  0.0440,  0.2310]],\n",
       "\n",
       "        [[-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.1704,  0.2031,  0.1450,  0.0516, -0.1007,  0.2065, -0.1367,\n",
       "           0.0156,  0.0407,  0.2146],\n",
       "         [-0.1703,  0.2269,  0.1584,  0.0454, -0.1002,  0.2149, -0.1540,\n",
       "           0.0192,  0.0440,  0.2310]],\n",
       "\n",
       "        [[-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.2272,  0.2708,  0.1933,  0.0689, -0.1342,  0.2754, -0.1823,\n",
       "           0.0209,  0.0543,  0.2861],\n",
       "         [-0.1704,  0.2031,  0.1450,  0.0516, -0.1007,  0.2065, -0.1367,\n",
       "           0.0156,  0.0407,  0.2146],\n",
       "         [-0.1703,  0.2269,  0.1584,  0.0454, -0.1002,  0.2149, -0.1540,\n",
       "           0.0192,  0.0440,  0.2310]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = torch.ones((3,5,10),dtype=torch.float32)\n",
    "test_vec[:,3]=0\n",
    "tester(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed !\n"
     ]
    }
   ],
   "source": [
    "tester.compare_forwards(test_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
